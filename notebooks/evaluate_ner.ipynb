{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import pipeline\n",
    "import copy\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import argparse\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "print(load_dotenv(find_dotenv(\".env\")))\n",
    "\n",
    "import regex  # Note: This is NOT the built-in 're' module\n",
    "\n",
    "def split_sentence_with_indices(text):\n",
    "    pattern = r'''\n",
    "        (?:\n",
    "            \\p{N}+[.,]?\\p{N}*\\s*[%$€]?           # Numbers with optional decimal/currency\n",
    "        )\n",
    "        |\n",
    "        \\p{L}+(?:-\\p{L}+)*                      # Words with optional hyphens (letters from any language)\n",
    "        |\n",
    "        [()\\[\\]{}]                              # Parentheses and brackets\n",
    "        |\n",
    "        [^\\p{L}\\p{N}\\s]                         # Other single punctuation marks\n",
    "    '''\n",
    "    return list(regex.finditer(pattern, text, flags=regex.VERBOSE))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def write_annotations_to_file(data, file_path):\n",
    "    \"\"\"\n",
    "    Writes annotation data to a TSV file.\n",
    "\n",
    "    Parameters:\n",
    "        data (list of dict): Each dict should have keys:\n",
    "            'filename', 'ann_id', 'label', 'start_span', 'end_span', 'text'\n",
    "        file_path (str): Path to the output file\n",
    "    \"\"\"\n",
    "    header = ['filename', 'ann_id', 'label', 'start_span', 'end_span', 'text']\n",
    "\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        # Write the header\n",
    "        f.write('\\t'.join(header) + '\\n')\n",
    "        # Write each row\n",
    "        for entry in data:\n",
    "            row = [str(entry[key]) for key in header]\n",
    "            f.write('\\t'.join(row) + '\\n')\n",
    "\n",
    "\n",
    "def load_tsv_to_dataframe(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads a TSV file with specific columns into a pandas DataFrame.\n",
    "\n",
    "    Expected columns:\n",
    "        filename, label, start_span, end_span, text, note\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the TSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the TSV data.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(\n",
    "        file_path,\n",
    "        sep='\\t',\n",
    "        dtype={\n",
    "            \"filename\": str,\n",
    "            \"label\": str,\n",
    "            \"start_span\": int,\n",
    "            \"end_span\": int,\n",
    "            \"text\": str,\n",
    "            \"note\": str\n",
    "        },\n",
    "        keep_default_na=False  # Prevents empty strings being converted to NaN\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "class PredictionNER:\n",
    "    def __init__(self, model_checkpoint, revision) -> None:\n",
    "        MAX_TOKENS_IOB_SENT = 256\n",
    "        OVERLAPPING_LEN = 0\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_checkpoint, revision=revision, is_split_into_words=True, truncation=False\n",
    "        )\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(\n",
    "            model_checkpoint, revision=revision\n",
    "        )\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "            encoding_name='o200k_base',\n",
    "            separators=[\"\\n\\n\\n\", \"\\n\\n\", \"\\n\", \" .\", \" !\", \" ?\", \" ،\", \" ,\", \" \", \"\"],\n",
    "            keep_separator=True,\n",
    "            chunk_size=MAX_TOKENS_IOB_SENT,\n",
    "            chunk_overlap=OVERLAPPING_LEN,\n",
    "        )\n",
    "\n",
    "\n",
    "        # Use a pipeline as a high-level helper\n",
    "\n",
    "        self.pipe = pipeline(\"token-classification\", model=model_checkpoint, revision=revision, aggregation_strategy=\"average\")\n",
    "\n",
    "        ner_labels = list(self.model.config.id2label.values())\n",
    "        self.base_entity_types = sorted(\n",
    "            set(label[2:] for label in ner_labels if label != \"O\")\n",
    "        )\n",
    "\n",
    "\n",
    "    def split_text_with_indices(self, text):\n",
    "        \n",
    "        raw_chunks = self.text_splitter.split_text(text)\n",
    "\n",
    "        # Align each chunk manually by finding its first occurrence in text\n",
    "        used_indices = set()\n",
    "        # chunks = []\n",
    "\n",
    "        for chunk_text in raw_chunks:\n",
    "            # Find the first unique match position in text to use as a start index\n",
    "            start_index = text.find(chunk_text)\n",
    "\n",
    "            # Prevent collisions if chunk_text repeats (naïve fallback)\n",
    "            while start_index in used_indices:\n",
    "                start_index = text.find(chunk_text, start_index + 1)\n",
    "            used_indices.add(start_index)\n",
    "\n",
    "            end_index = start_index + len(chunk_text)\n",
    "            \n",
    "            yield chunk_text, start_index, end_index\n",
    "            \n",
    "\n",
    "    def predict_text(self, text: str, o_confidence_threshold: float = 0.70):\n",
    "        # 1. Split text into words and punctuation using regex\n",
    "        text_matches = split_sentence_with_indices(text)\n",
    "\n",
    "        # 2. Strip and filter out empty or whitespace-only tokens\n",
    "        text_words = [m.group().strip() for m in text_matches if m.group().strip()]\n",
    "\n",
    "        if not text_words:\n",
    "            return []\n",
    "\n",
    "        # 3. Tokenize with word alignment\n",
    "        inputs = self.tokenizer(\n",
    "            text_words,\n",
    "            return_tensors=\"pt\",\n",
    "            is_split_into_words=True,\n",
    "            truncation=False\n",
    "        )\n",
    "        word_ids = inputs.word_ids()\n",
    "\n",
    "        # 4. Predict\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(**inputs).logits\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        predictions = torch.argmax(logits, dim=2)[0]\n",
    "\n",
    "        # 5. Map predictions back to original stripped words\n",
    "        results = []\n",
    "        seen = set()\n",
    "        non_empty_matches = [m for m in text_matches if m.group().strip()]\n",
    "        id2label = self.model.config.id2label\n",
    "\n",
    "        for i, word_idx in enumerate(word_ids):\n",
    "            if word_idx is None or word_idx in seen:\n",
    "                continue\n",
    "            seen.add(word_idx)\n",
    "\n",
    "            word = text_words[word_idx]\n",
    "            start = non_empty_matches[word_idx].start()\n",
    "            end = non_empty_matches[word_idx].end()\n",
    "\n",
    "            tag_id = predictions[i].item()\n",
    "            tag = id2label[tag_id]\n",
    "            score = probs[0, i, tag_id].item()\n",
    "\n",
    "            # If the tag is \"O\" and its confidence is low, try to find the next best non-\"O\" label\n",
    "            if tag == \"O\" and score < o_confidence_threshold:\n",
    "                sorted_probs = torch.argsort(probs[0, i], descending=True)\n",
    "                for alt_id in sorted_probs:\n",
    "                    alt_tag = id2label[alt_id.item()]\n",
    "                    if alt_tag != \"O\":\n",
    "                        tag_id = alt_id.item()\n",
    "                        tag = alt_tag\n",
    "                        score = probs[0, i, tag_id].item()\n",
    "                        break  # take the first non-\"O\" alternative\n",
    "\n",
    "            results.append({\n",
    "                'word': word,\n",
    "                'tag': tag,\n",
    "                'start': start,\n",
    "                'end': end,\n",
    "                'score': score\n",
    "            })\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def aggregate_entities(self, tagged_tokens, original_text, confidence_threshold=0.3):\n",
    "        def is_special_char(text):\n",
    "            return bool(re.fullmatch(r\"\\W+\", text.strip()))\n",
    "\n",
    "        def finalize_entity(entity):\n",
    "            if all(s >= confidence_threshold for s in entity[\"scores\"]):\n",
    "                entity_text = original_text[entity[\"start\"]:entity[\"end\"]]\n",
    "                if not is_special_char(entity_text):\n",
    "                    entity[\"text\"] = entity_text\n",
    "                    entity[\"score\"] = sum(entity[\"scores\"]) / len(entity[\"scores\"])\n",
    "                    del entity[\"scores\"]\n",
    "                    return entity\n",
    "            return None\n",
    "\n",
    "        corrected_tokens = copy.deepcopy(tagged_tokens)\n",
    "\n",
    "        # Rule 1: Fix \"O\" between \"B-\" and \"I-\" of same type\n",
    "        for i in range(1, len(corrected_tokens) - 1):\n",
    "            prev_tag = corrected_tokens[i - 1][\"tag\"]\n",
    "            curr_tag = corrected_tokens[i][\"tag\"]\n",
    "            next_tag = corrected_tokens[i + 1][\"tag\"]\n",
    "\n",
    "            if curr_tag == \"O\" and prev_tag.startswith(\"B-\") and next_tag.startswith(\"I-\"):\n",
    "                prev_type = prev_tag[2:]\n",
    "                next_type = next_tag[2:]\n",
    "                if prev_type == next_type:\n",
    "                    corrected_tokens[i][\"tag\"] = \"I-\" + prev_type\n",
    "\n",
    "        # Rule 2: Convert isolated I- to B-\n",
    "        last_tag_type = None\n",
    "        for i in range(len(corrected_tokens)):\n",
    "            tag = corrected_tokens[i][\"tag\"]\n",
    "            if tag.startswith(\"I-\"):\n",
    "                tag_type = tag[2:]\n",
    "                if last_tag_type != tag_type:\n",
    "                    corrected_tokens[i][\"tag\"] = \"B-\" + tag_type\n",
    "                last_tag_type = tag_type\n",
    "            elif tag.startswith(\"B-\"):\n",
    "                last_tag_type = tag[2:]\n",
    "            else:\n",
    "                last_tag_type = None\n",
    "\n",
    "        # Step 2: Aggregate entities\n",
    "        entities = []\n",
    "        current_entity = None\n",
    "\n",
    "        for idx, item in enumerate(corrected_tokens):\n",
    "            tag = item[\"tag\"]\n",
    "            start = item[\"start\"]\n",
    "            end = item[\"end\"]\n",
    "            score = item[\"score\"]\n",
    "\n",
    "            if tag.startswith(\"B-\"):\n",
    "                if current_entity:\n",
    "                    # Check if current should be merged (same type and touching or separated by only whitespace)\n",
    "                    if (current_entity[\"tag\"] == tag[2:] and \n",
    "                        (current_entity[\"end\"] == start or\n",
    "                        original_text[current_entity[\"end\"]:start].isspace())):\n",
    "                        # Merge\n",
    "                        current_entity[\"end\"] = end\n",
    "                        current_entity[\"scores\"].append(score)\n",
    "                        continue\n",
    "                    else:\n",
    "                        finalized = finalize_entity(current_entity)\n",
    "                        if finalized:\n",
    "                            entities.append(finalized)\n",
    "                current_entity = {\n",
    "                    \"start\": start,\n",
    "                    \"end\": end,\n",
    "                    \"tag\": tag[2:],\n",
    "                    \"scores\": [score]\n",
    "                }\n",
    "\n",
    "            elif tag.startswith(\"I-\"):\n",
    "                tag_type = tag[2:]\n",
    "                if current_entity and current_entity[\"tag\"] == tag_type:\n",
    "                    current_entity[\"end\"] = end\n",
    "                    current_entity[\"scores\"].append(score)\n",
    "                else:\n",
    "                    current_entity = {\n",
    "                        \"start\": start,\n",
    "                        \"end\": end,\n",
    "                        \"tag\": tag_type,\n",
    "                        \"scores\": [score]\n",
    "                    }\n",
    "\n",
    "            else:  # tag == \"O\"\n",
    "                if current_entity:\n",
    "                    finalized = finalize_entity(current_entity)\n",
    "                    if finalized:\n",
    "                        entities.append(finalized)\n",
    "                    current_entity = None\n",
    "\n",
    "        # Finalize last entity\n",
    "        if current_entity:\n",
    "            finalized = finalize_entity(current_entity)\n",
    "            if finalized:\n",
    "                entities.append(finalized)\n",
    "\n",
    "        return entities\n",
    "\n",
    "\n",
    "\n",
    "    def do_prediction(self, text, confidence_threshold=0.6):\n",
    "        final_prediction = []\n",
    "        # final_prediction_2 = []\n",
    "        for sub_text, sub_text_start, sub_text_end in self.split_text_with_indices(text):\n",
    "            tokens = self.predict_text(text=sub_text)\n",
    "            predictions = self.aggregate_entities(tokens, sub_text, confidence_threshold=confidence_threshold)\n",
    "\n",
    "            for pred in predictions:\n",
    "                pred[\"start\"] += sub_text_start\n",
    "                pred[\"end\"] += sub_text_start\n",
    "                final_prediction.append(pred)\n",
    "                \n",
    "                \n",
    "\n",
    "        return final_prediction\n",
    "\n",
    "\n",
    "def evaluate(model_checkpoint, revision, root_path, lang, cat):\n",
    "\n",
    "    ner = PredictionNER(model_checkpoint=model_checkpoint, revision=revision)\n",
    "\n",
    "    # conver the predictions to ann format\n",
    "    test_files_root =  os.path.join(root_path, \"txt\")\n",
    "    tsv_file_path_test = os.path.join(root_path, f\"test_cardioccc_{lang}_{cat}.tsv\")\n",
    "    test_df = load_tsv_to_dataframe(tsv_file_path_test)\n",
    "    prd_ann = []\n",
    "\n",
    "    for fn in tqdm(test_df['filename'].unique()):\n",
    "        # fn = \"casos_clinicos_cardiologia508\"\n",
    "        with open(os.path.join(test_files_root, fn+\".txt\"), 'r', encoding='utf-8') as f:\n",
    "            document_text = f.read()\n",
    "            prds = ner.do_prediction(document_text, confidence_threshold=0.35)\n",
    "            for prd in prds:\n",
    "                prd_ann.append({\n",
    "                    \"filename\": fn,\n",
    "                    \"label\": prd[\"tag\"],\n",
    "                    \"ann_id\": \"NA\",\n",
    "                    \"start_span\": prd[\"start\"],\n",
    "                    \"end_span\": prd[\"end\"],\n",
    "                    \"text\": prd[\"text\"],\n",
    "                })\n",
    "        # break \n",
    "\n",
    "    output_tsv_path = os.path.join(root_path, f\"pre_{model_checkpoint.split('/')[1]}_{revision}.tsv\")\n",
    "    write_annotations_to_file(prd_ann, output_tsv_path)\n",
    "    print(f\"output_tsv_path {output_tsv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "100%|██████████| 223/223 [13:01<00:00,  3.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_tsv_path ../dataset/Romanian/pre_CardioBERTa.ro_RO_MED_2025-05-19_09-09-56-c0c818ad.tsv\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"DT4H-IE/CardioBERTa.ro_RO_MED\"\n",
    "revision = \"2025-05-19_09-09-56-c0c818ad\"\n",
    "root = \"../dataset/Romanian/\" \n",
    "cat = \"med\"\n",
    "lang = \"ro\"\n",
    "\n",
    "evaluate(model_checkpoint, revision, root, lang, cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To load pre-trained model training configs:\n",
    "\n",
    "# arg_path  = r\"./trained_models/model_bert-base-spanish-wwm-cased_es_med_30042025_06bc94a6/training_args.bin\"\n",
    "\n",
    "# import torch\n",
    "\n",
    "# # Load the full object (not just weights)\n",
    "# training_args = torch.load(arg_path, weights_only=False)\n",
    "\n",
    "# print(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
