{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import re\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# import re\n",
    "# from tqdm import tqdm\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# import pandas as pd\n",
    "# from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "# import re\n",
    "\n",
    "# def split_sentence_with_indices(text):\n",
    "#     pattern = r'''\n",
    "#         (?:\n",
    "#             \\d+[.,]?\\d*\\s*[%$€]?\n",
    "#         )                                     # Numbers with optional decimal/currency\n",
    "#         |\n",
    "#         [A-Za-zÀ-ÖØ-öø-ÿ0-9]+(?:-[A-Za-z0-9]+)*  # Words with optional hyphens (e.g., anti-TNF)\n",
    "#         |\n",
    "#         [()\\[\\]{}]                             # Parentheses and brackets\n",
    "#         |\n",
    "#         [^\\w\\s]                                # Other single punctuation marks\n",
    "#     '''\n",
    "#     return list(re.finditer(pattern, text, flags=re.VERBOSE))\n",
    "\n",
    "\n",
    "\n",
    "# def write_annotations_to_file(data, file_path):\n",
    "#     \"\"\"\n",
    "#     Writes annotation data to a TSV file.\n",
    "\n",
    "#     Parameters:\n",
    "#         data (list of dict): Each dict should have keys:\n",
    "#             'filename', 'ann_id', 'label', 'start_span', 'end_span', 'text'\n",
    "#         file_path (str): Path to the output file\n",
    "#     \"\"\"\n",
    "#     header = ['filename', 'ann_id', 'label', 'start_span', 'end_span', 'text']\n",
    "\n",
    "#     with open(file_path, 'w', encoding='utf-8') as f:\n",
    "#         # Write the header\n",
    "#         f.write('\\t'.join(header) + '\\n')\n",
    "#         # Write each row\n",
    "#         for entry in data:\n",
    "#             row = [str(entry[key]) for key in header]\n",
    "#             f.write('\\t'.join(row) + '\\n')\n",
    "\n",
    "\n",
    "# def load_tsv_to_dataframe(file_path: str) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Loads a TSV file with specific columns into a pandas DataFrame.\n",
    "\n",
    "#     Expected columns:\n",
    "#         filename, label, start_span, end_span, text, note\n",
    "\n",
    "#     Args:\n",
    "#         file_path (str): Path to the TSV file.\n",
    "\n",
    "#     Returns:\n",
    "#         pd.DataFrame: DataFrame containing the TSV data.\n",
    "#     \"\"\"\n",
    "#     df = pd.read_csv(\n",
    "#         file_path,\n",
    "#         sep='\\t',\n",
    "#         dtype={\n",
    "#             \"filename\": str,\n",
    "#             \"label\": str,\n",
    "#             \"start_span\": int,\n",
    "#             \"end_span\": int,\n",
    "#             \"text\": str,\n",
    "#             \"note\": str\n",
    "#         },\n",
    "#         keep_default_na=False  # Prevents empty strings being converted to NaN\n",
    "#     )\n",
    "#     return df\n",
    "\n",
    "\n",
    "# class PredictionNER:\n",
    "#     def __init__(self, model_checkpoint, revision) -> None:\n",
    "#         MAX_LENGTH = 500\n",
    "#         OVERLAPPING_LEN = 12 \n",
    "\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "#             model_checkpoint, revision=revision, is_split_into_words=True\n",
    "#         )\n",
    "#         self.model = AutoModelForTokenClassification.from_pretrained(\n",
    "#             model_checkpoint, revision=revision\n",
    "#         )\n",
    "#         self.text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "#             encoding_name='o200k_base',\n",
    "#             separators=[\"\\n\\n\\n\", \"\\n\\n\", \"\\n\", \".\", \",\", \" \", \"\"],\n",
    "#             keep_separator=False,\n",
    "#             chunk_size=MAX_LENGTH,\n",
    "#             chunk_overlap=OVERLAPPING_LEN,\n",
    "#         )\n",
    "\n",
    "#         ner_labels = list(self.model.config.id2label.values())\n",
    "#         self.base_entity_types = sorted(\n",
    "#             set(label[2:] for label in ner_labels if label != \"O\")\n",
    "#         )\n",
    "\n",
    "\n",
    "#     def split_text_with_indices(self, text):\n",
    "#         offset = 0\n",
    "#         for doc in self.text_splitter.split_text(text):\n",
    "#             # Search for doc within the remaining text\n",
    "#             start_idx = text.find(doc, offset)\n",
    "#             if start_idx == -1:\n",
    "#                 continue  # should not happen, but skip just in case\n",
    "#             end_idx = start_idx + len(doc)\n",
    "#             offset = end_idx  # move search window forward\n",
    "#             yield doc, start_idx, end_idx\n",
    "\n",
    "#     def predict_text(self, text: str, confidence_threshold: float = 0.7):\n",
    "#         # 1. Split text into words and punctuation using regex\n",
    "#         text_matches = split_sentence_with_indices(text)  # list(re.finditer(r'([0-9A-Za-zÀ-ÖØ-öø-ÿ]+|[^0-9A-Za-zÀ-ÖØ-öø-ÿ])', text))\n",
    "\n",
    "#         # 2. Strip and filter out empty or whitespace-only tokens\n",
    "#         text_words = [m.group().strip() for m in text_matches if m.group().strip()]\n",
    "\n",
    "#         if not text_words:\n",
    "#             return []  # return early if nothing valid\n",
    "\n",
    "#         # 3. Tokenize with word alignment\n",
    "#         inputs = self.tokenizer(\n",
    "#             text_words, \n",
    "#             return_tensors=\"pt\", \n",
    "#             is_split_into_words=True,\n",
    "#             truncation=False\n",
    "#         )\n",
    "#         word_ids = inputs.word_ids()\n",
    "\n",
    "#         # 4. Predict\n",
    "#         with torch.no_grad():\n",
    "#             logits = self.model(**inputs).logits\n",
    "#             probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "#         predictions = torch.argmax(logits, dim=2)[0]\n",
    "\n",
    "#         # 5. Map predictions back to original stripped words\n",
    "#         results = []\n",
    "#         seen = set()\n",
    "#         non_empty_matches = [m for m in text_matches if m.group().strip()]\n",
    "\n",
    "#         for i, word_idx in enumerate(word_ids):\n",
    "#             if word_idx is None or word_idx in seen:\n",
    "#                 continue\n",
    "#             seen.add(word_idx)\n",
    "\n",
    "#             word = text_words[word_idx]\n",
    "#             tag_id = predictions[i].item()\n",
    "#             tag = self.model.config.id2label[tag_id]\n",
    "#             score = probs[0, i, tag_id].item()\n",
    "#             start = non_empty_matches[word_idx].start()\n",
    "#             end = non_empty_matches[word_idx].end()\n",
    "\n",
    "#             # Apply the confidence threshold filter\n",
    "#             if score < confidence_threshold:\n",
    "#                 tag = \"O\"  # Assign \"O\" tag if confidence is below threshold\n",
    "#                 score = 0.0  # Set score to 0 for \"O\" tag\n",
    "\n",
    "#             results.append({\n",
    "#                 'word': word,\n",
    "#                 'tag': tag,\n",
    "#                 'start': start,\n",
    "#                 'end': end,\n",
    "#                 'score': score\n",
    "#             })\n",
    "\n",
    "#         return results\n",
    "\n",
    "#     def aggregate_entities(self, tagged_tokens, original_text, confidence_threshold=0.3):\n",
    "#         # Step 1: Preprocess tags based on the two rules\n",
    "#         corrected_tokens = tagged_tokens.copy()\n",
    "\n",
    "#         # Rule 1: Fix \"O\" between \"B-\" and \"I-\" of the same type\n",
    "#         for i in range(1, len(tagged_tokens) - 1):\n",
    "#             prev_tag = tagged_tokens[i - 1][\"tag\"]\n",
    "#             curr_tag = tagged_tokens[i][\"tag\"]\n",
    "#             next_tag = tagged_tokens[i + 1][\"tag\"]\n",
    "\n",
    "#             if (\n",
    "#                 curr_tag == \"O\" and\n",
    "#                 prev_tag.startswith(\"B-\") and\n",
    "#                 next_tag.startswith(\"I-\")\n",
    "#             ):\n",
    "#                 prev_type = prev_tag[2:]\n",
    "#                 next_type = next_tag[2:]\n",
    "#                 if prev_type == next_type:\n",
    "#                     corrected_tokens[i][\"tag\"] = \"I-\" + prev_type\n",
    "\n",
    "#         # Rule 2: Convert isolated or starting I- to B-\n",
    "#         last_tag_type = None\n",
    "#         for i in range(len(corrected_tokens)):\n",
    "#             tag = corrected_tokens[i][\"tag\"]\n",
    "#             if tag.startswith(\"I-\"):\n",
    "#                 tag_type = tag[2:]\n",
    "#                 if last_tag_type != tag_type:\n",
    "#                     corrected_tokens[i][\"tag\"] = \"B-\" + tag_type\n",
    "#                     last_tag_type = tag_type\n",
    "#                 else:\n",
    "#                     last_tag_type = tag_type\n",
    "#             elif tag.startswith(\"B-\"):\n",
    "#                 last_tag_type = tag[2:]\n",
    "#             else:\n",
    "#                 last_tag_type = None\n",
    "\n",
    "#         # Step 2: Apply original aggregation logic\n",
    "#         entities = []\n",
    "#         current_entity = None\n",
    "\n",
    "#         for item in corrected_tokens:\n",
    "#             tag = item[\"tag\"]\n",
    "#             start = item[\"start\"]\n",
    "#             end = item[\"end\"]\n",
    "#             score = item[\"score\"]\n",
    "\n",
    "#             if tag.startswith(\"B-\"):\n",
    "#                 if current_entity:\n",
    "#                     if all(s >= confidence_threshold for s in current_entity[\"scores\"]):\n",
    "#                         current_entity[\"text\"] = original_text[current_entity[\"start\"]:current_entity[\"end\"]]\n",
    "#                         current_entity[\"score\"] = sum(current_entity[\"scores\"]) / len(current_entity[\"scores\"])\n",
    "#                         del current_entity[\"scores\"]\n",
    "#                         entities.append(current_entity)\n",
    "#                     current_entity = None\n",
    "#                 tag_type = tag[2:]\n",
    "#                 current_entity = {\n",
    "#                     \"start\": start,\n",
    "#                     \"end\": end,\n",
    "#                     \"tag\": tag_type,\n",
    "#                     \"scores\": [score]\n",
    "#                 }\n",
    "\n",
    "#             elif tag.startswith(\"I-\"):\n",
    "#                 tag_type = tag[2:]\n",
    "#                 if current_entity and current_entity[\"tag\"] == tag_type:\n",
    "#                     current_entity[\"end\"] = end\n",
    "#                     current_entity[\"scores\"].append(score)\n",
    "#                 else:\n",
    "#                     current_entity = {\n",
    "#                         \"start\": start,\n",
    "#                         \"end\": end,\n",
    "#                         \"tag\": tag_type,\n",
    "#                         \"scores\": [score]\n",
    "#                     }\n",
    "\n",
    "#             else:  # \"O\"\n",
    "#                 if current_entity:\n",
    "#                     if all(s >= confidence_threshold for s in current_entity[\"scores\"]):\n",
    "#                         current_entity[\"text\"] = original_text[current_entity[\"start\"]:current_entity[\"end\"]]\n",
    "#                         current_entity[\"score\"] = sum(current_entity[\"scores\"]) / len(current_entity[\"scores\"])\n",
    "#                         del current_entity[\"scores\"]\n",
    "#                         entities.append(current_entity)\n",
    "#                     current_entity = None\n",
    "\n",
    "#         if current_entity:\n",
    "#             if all(s >= confidence_threshold for s in current_entity[\"scores\"]):\n",
    "#                 current_entity[\"text\"] = original_text[current_entity[\"start\"]:current_entity[\"end\"]]\n",
    "#                 current_entity[\"score\"] = sum(current_entity[\"scores\"]) / len(current_entity[\"scores\"])\n",
    "#                 del current_entity[\"scores\"]\n",
    "#                 entities.append(current_entity)\n",
    "\n",
    "#         return entities\n",
    "\n",
    "\n",
    "#     def do_prediction(self, text, confidence_threshold=0.6):\n",
    "#         final_prediction = []\n",
    "#         for sub_text, sub_text_start, sub_text_end in self.split_text_with_indices(text):\n",
    "#             tokens = self.predict_text(text=sub_text, confidence_threshold=confidence_threshold)\n",
    "#             predictions = self.aggregate_entities(tokens, sub_text, confidence_threshold=confidence_threshold)\n",
    "\n",
    "\n",
    "#             for pred in predictions:\n",
    "#                 pred[\"start\"] += sub_text_start\n",
    "#                 pred[\"end\"] += sub_text_start\n",
    "#                 final_prediction.append(pred)\n",
    "\n",
    "#         final_prediction_dict = {\n",
    "#             lab: [p for p in final_prediction if p[\"tag\"] == lab]\n",
    "#             for lab in self.base_entity_types\n",
    "#         }\n",
    "#         merged_predictions = []\n",
    "#         for label in self.base_entity_types:\n",
    "#             merged_predictions.extend(final_prediction_dict[label])\n",
    "#         return merged_predictions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"DT4H-IE/CardioBERTa.es_ES_MED\"\n",
    "revision = \"2025-05-06_09-40-29-ed3a39e1\" \n",
    "\n",
    "root = \"../dataset/Spanish\"\n",
    "lang = \"es\"\n",
    "cat = \"med\"\n",
    "\n",
    "ner = PredictionNER(model_checkpoint=model_checkpoint, revision=revision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/219 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['atorvastatina',\n",
       " 'estatina',\n",
       " 'atenolol',\n",
       " 'simvastatina',\n",
       " 'atorvastatina',\n",
       " 'ácido acetilsalicílico',\n",
       " 'bisoprolol',\n",
       " 'ramipril',\n",
       " 'subtilisina/',\n",
       " 'evolocumab',\n",
       " 'atorvastatina']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# conver the predictions to ann format\n",
    "tsv_file_path_test = os.path.join(root,  f\"test_cardioccc_{lang}_{cat}.tsv\")\n",
    "test_files_root =  os.path.join(root, \"txt\")\n",
    "\n",
    "test_df = load_tsv_to_dataframe(tsv_file_path_test)\n",
    "prd_ann = []\n",
    "\n",
    "for fn in tqdm(test_df['filename'].unique()):\n",
    "\n",
    "    with open(os.path.join(test_files_root, fn+\".txt\"), 'r', encoding='utf-8') as f:\n",
    "        document_text = f.read()\n",
    "        prds = ner.do_prediction(document_text, confidence_threshold=0.35)\n",
    "        for prd in prds:\n",
    "            prd_ann.append({\n",
    "                \"filename\": fn,\n",
    "                \"label\": prd[\"tag\"],\n",
    "                \"ann_id\": \"NA\",\n",
    "                \"start_span\": prd[\"start\"],\n",
    "                \"end_span\": prd[\"end\"],\n",
    "                \"text\": prd[\"text\"],\n",
    "            })\n",
    "\n",
    "# [x['text'] for x in prd_ann]\n",
    "write_annotations_to_file(prd_ann, os.path.join(root, f\"pre_{model_checkpoint.split('/')[1]}_{revision}.tsv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 315,\n",
       "  'end': 327,\n",
       "  'tag': 'MEDICATION',\n",
       "  'text': 'simvastatina',\n",
       "  'score': 0.9982998967170715},\n",
       " {'start': 386,\n",
       "  'end': 399,\n",
       "  'tag': 'MEDICATION',\n",
       "  'text': 'atorvastatina',\n",
       "  'score': 0.9982932209968567},\n",
       " {'start': 1165,\n",
       "  'end': 1187,\n",
       "  'tag': 'MEDICATION',\n",
       "  'text': 'ácido acetilsalicílico',\n",
       "  'score': 0.9945134520530701},\n",
       " {'start': 1213,\n",
       "  'end': 1223,\n",
       "  'tag': 'MEDICATION',\n",
       "  'text': 'bisoprolol',\n",
       "  'score': 0.9983952641487122},\n",
       " {'start': 1235,\n",
       "  'end': 1243,\n",
       "  'tag': 'MEDICATION',\n",
       "  'text': 'ramipril',\n",
       "  'score': 0.9983286261558533},\n",
       " {'start': 1311,\n",
       "  'end': 1320,\n",
       "  'tag': 'MEDICATION',\n",
       "  'text': 'inhibidor',\n",
       "  'score': 0.6627407670021057},\n",
       " {'start': 1350,\n",
       "  'end': 1363,\n",
       "  'tag': 'MEDICATION',\n",
       "  'text': 'subtilisina /',\n",
       "  'score': 0.6465933620929718},\n",
       " {'start': 1388,\n",
       "  'end': 1398,\n",
       "  'tag': 'MEDICATION',\n",
       "  'text': 'evolocumab',\n",
       "  'score': 0.9982590079307556},\n",
       " {'start': 1513,\n",
       "  'end': 1526,\n",
       "  'tag': 'MEDICATION',\n",
       "  'text': 'atorvastatina',\n",
       "  'score': 0.9983083009719849}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"EVOLUCIÓN CLÍNICA\n",
    "Paciente con antecedentes familiares de enfermedad coronaria precoz y sospecha de hipercolesterolemia familiar, asintomático desde el punto de vista cardiológico hasta el episodio actual, altamente sugestivo de angina de esfuerzo. Años antes había recibido tratamiento con estatinas, inicialmente simvastatina 20 pero se sustituyó por ineficacia (cLDL > 180 mg/d) por atorvastatina 10 que tuvo que suspenderse por daño hepático, reversible al interrumpir el tratamiento. Esto ocurrió en el año 2009 (en ese momento había bajado a LDL 129 mg/dl). Desde entonces, sin tratamiento hipolipemiante, ha mantenido niveles de cLDL entre 180 y 250 mg/dl. En la actualidad, consulta por un único dolor torácico en relación a un esfuerzo mayor de lo habitual; se descarta evento agudo y se realiza ergometría que es negativa 12 MET, pero debido a la probabilidad de enfermedad coronaria (síntomas típicos, historia familiar e hipercolesterolemia mantenida durante muchos años) se solicita angio-TC coronario, en el que se aprecian varias estenosis no significativas en el árbol coronario. A la vista de estos resultados, se inicia tratamiento antiagregante (ácido acetilsalicílico 100 mg), betabloqueante (bisoprolol 2,5 mg ) y ramipril (2,5 mg), además de tratamiento hipolipemiante en este caso con un inhibidor de la proproteína convertasa subtilisina / kexina tipo 9 (iPCSK9) (evolocumab 140 mg/2 semanas) por el antecedente de probable hipercolesterolemia familiar, intolerancia a la dosis inicial de atorvastatina e ineficacia con otra estatina menos potente y la presencia de enfermedad coronaria. Actualmente se encuentra pendiente de resultado de estudio genético.\n",
    "\"\"\"\n",
    "ner.do_prediction(text,0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load pre-trained model training configs:\n",
    "\n",
    "arg_path  = r\"./trained_models/model_bert-base-spanish-wwm-cased_es_med_30042025_06bc94a6/training_args.bin\"\n",
    "\n",
    "import torch\n",
    "\n",
    "# Load the full object (not just weights)\n",
    "training_args = torch.load(arg_path, weights_only=False)\n",
    "\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import argparse\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "print(load_dotenv(find_dotenv(\".env\")))\n",
    "\n",
    "def split_sentence_with_indices(text):\n",
    "    pattern = r'''\n",
    "        (?:\n",
    "            \\d+[.,]?\\d*\\s*[%$€]?\n",
    "        )                                     # Numbers with optional decimal/currency\n",
    "        |\n",
    "        [A-Za-zÀ-ÖØ-öø-ÿ0-9]+(?:-[A-Za-z0-9]+)*  # Words with optional hyphens (e.g., anti-TNF)\n",
    "        |\n",
    "        [()\\[\\]{}]                             # Parentheses and brackets\n",
    "        |\n",
    "        [^\\w\\s]                                # Other single punctuation marks\n",
    "    '''\n",
    "    return list(re.finditer(pattern, text, flags=re.VERBOSE))\n",
    "\n",
    "\n",
    "\n",
    "def write_annotations_to_file(data, file_path):\n",
    "    \"\"\"\n",
    "    Writes annotation data to a TSV file.\n",
    "\n",
    "    Parameters:\n",
    "        data (list of dict): Each dict should have keys:\n",
    "            'filename', 'ann_id', 'label', 'start_span', 'end_span', 'text'\n",
    "        file_path (str): Path to the output file\n",
    "    \"\"\"\n",
    "    header = ['filename', 'ann_id', 'label', 'start_span', 'end_span', 'text']\n",
    "\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        # Write the header\n",
    "        f.write('\\t'.join(header) + '\\n')\n",
    "        # Write each row\n",
    "        for entry in data:\n",
    "            row = [str(entry[key]) for key in header]\n",
    "            f.write('\\t'.join(row) + '\\n')\n",
    "\n",
    "\n",
    "def load_tsv_to_dataframe(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads a TSV file with specific columns into a pandas DataFrame.\n",
    "\n",
    "    Expected columns:\n",
    "        filename, label, start_span, end_span, text, note\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the TSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the TSV data.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(\n",
    "        file_path,\n",
    "        sep='\\t',\n",
    "        dtype={\n",
    "            \"filename\": str,\n",
    "            \"label\": str,\n",
    "            \"start_span\": int,\n",
    "            \"end_span\": int,\n",
    "            \"text\": str,\n",
    "            \"note\": str\n",
    "        },\n",
    "        keep_default_na=False  # Prevents empty strings being converted to NaN\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "class PredictionNER:\n",
    "    def __init__(self, model_checkpoint, revision) -> None:\n",
    "        MAX_LENGTH = 500\n",
    "        OVERLAPPING_LEN = 12 \n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_checkpoint, revision=revision, is_split_into_words=True\n",
    "        )\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(\n",
    "            model_checkpoint, revision=revision\n",
    "        )\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "            encoding_name='o200k_base',\n",
    "            separators=[\"\\n\\n\\n\", \"\\n\\n\", \"\\n\", \".\", \",\", \" \", \"\"],\n",
    "            keep_separator=False,\n",
    "            chunk_size=MAX_LENGTH,\n",
    "            chunk_overlap=OVERLAPPING_LEN,\n",
    "        )\n",
    "\n",
    "        ner_labels = list(self.model.config.id2label.values())\n",
    "        self.base_entity_types = sorted(\n",
    "            set(label[2:] for label in ner_labels if label != \"O\")\n",
    "        )\n",
    "\n",
    "\n",
    "    def split_text_with_indices(self, text):\n",
    "        offset = 0\n",
    "        for doc in self.text_splitter.split_text(text):\n",
    "            # Search for doc within the remaining text\n",
    "            start_idx = text.find(doc, offset)\n",
    "            if start_idx == -1:\n",
    "                continue  # should not happen, but skip just in case\n",
    "            end_idx = start_idx + len(doc)\n",
    "            offset = end_idx  # move search window forward\n",
    "            yield doc, start_idx, end_idx\n",
    "\n",
    "    def predict_text(self, text: str, confidence_threshold: float = 0.7):\n",
    "        # 1. Split text into words and punctuation using regex\n",
    "        text_matches = split_sentence_with_indices(text)  # list(re.finditer(r'([0-9A-Za-zÀ-ÖØ-öø-ÿ]+|[^0-9A-Za-zÀ-ÖØ-öø-ÿ])', text))\n",
    "\n",
    "        # 2. Strip and filter out empty or whitespace-only tokens\n",
    "        text_words = [m.group().strip() for m in text_matches if m.group().strip()]\n",
    "\n",
    "        if not text_words:\n",
    "            return []  # return early if nothing valid\n",
    "\n",
    "        # 3. Tokenize with word alignment\n",
    "        inputs = self.tokenizer(\n",
    "            text_words, \n",
    "            return_tensors=\"pt\", \n",
    "            is_split_into_words=True,\n",
    "            truncation=False\n",
    "        )\n",
    "        word_ids = inputs.word_ids()\n",
    "\n",
    "        # 4. Predict\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(**inputs).logits\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        predictions = torch.argmax(logits, dim=2)[0]\n",
    "\n",
    "        # 5. Map predictions back to original stripped words\n",
    "        results = []\n",
    "        seen = set()\n",
    "        non_empty_matches = [m for m in text_matches if m.group().strip()]\n",
    "\n",
    "        for i, word_idx in enumerate(word_ids):\n",
    "            if word_idx is None or word_idx in seen:\n",
    "                continue\n",
    "            seen.add(word_idx)\n",
    "\n",
    "            word = text_words[word_idx]\n",
    "            tag_id = predictions[i].item()\n",
    "            tag = self.model.config.id2label[tag_id]\n",
    "            score = probs[0, i, tag_id].item()\n",
    "            start = non_empty_matches[word_idx].start()\n",
    "            end = non_empty_matches[word_idx].end()\n",
    "\n",
    "            # Apply the confidence threshold filter\n",
    "            if score < confidence_threshold:\n",
    "                tag = \"O\"  # Assign \"O\" tag if confidence is below threshold\n",
    "                score = 0.0  # Set score to 0 for \"O\" tag\n",
    "\n",
    "            results.append({\n",
    "                'word': word,\n",
    "                'tag': tag,\n",
    "                'start': start,\n",
    "                'end': end,\n",
    "                'score': score\n",
    "            })\n",
    "\n",
    "        return results\n",
    "\n",
    "    def aggregate_entities(self, tagged_tokens, original_text, confidence_threshold=0.3):\n",
    "        # Step 1: Preprocess tags based on the two rules\n",
    "        corrected_tokens = tagged_tokens.copy()\n",
    "\n",
    "        # Rule 1: Fix \"O\" between \"B-\" and \"I-\" of the same type\n",
    "        for i in range(1, len(tagged_tokens) - 1):\n",
    "            prev_tag = tagged_tokens[i - 1][\"tag\"]\n",
    "            curr_tag = tagged_tokens[i][\"tag\"]\n",
    "            next_tag = tagged_tokens[i + 1][\"tag\"]\n",
    "\n",
    "            if (\n",
    "                curr_tag == \"O\" and\n",
    "                prev_tag.startswith(\"B-\") and\n",
    "                next_tag.startswith(\"I-\")\n",
    "            ):\n",
    "                prev_type = prev_tag[2:]\n",
    "                next_type = next_tag[2:]\n",
    "                if prev_type == next_type:\n",
    "                    corrected_tokens[i][\"tag\"] = \"I-\" + prev_type\n",
    "\n",
    "        # Rule 2: Convert isolated or starting I- to B-\n",
    "        last_tag_type = None\n",
    "        for i in range(len(corrected_tokens)):\n",
    "            tag = corrected_tokens[i][\"tag\"]\n",
    "            if tag.startswith(\"I-\"):\n",
    "                tag_type = tag[2:]\n",
    "                if last_tag_type != tag_type:\n",
    "                    corrected_tokens[i][\"tag\"] = \"B-\" + tag_type\n",
    "                    last_tag_type = tag_type\n",
    "                else:\n",
    "                    last_tag_type = tag_type\n",
    "            elif tag.startswith(\"B-\"):\n",
    "                last_tag_type = tag[2:]\n",
    "            else:\n",
    "                last_tag_type = None\n",
    "\n",
    "        # Step 2: Apply original aggregation logic\n",
    "        entities = []\n",
    "        current_entity = None\n",
    "\n",
    "        for item in corrected_tokens:\n",
    "            tag = item[\"tag\"]\n",
    "            start = item[\"start\"]\n",
    "            end = item[\"end\"]\n",
    "            score = item[\"score\"]\n",
    "\n",
    "            if tag.startswith(\"B-\"):\n",
    "                if current_entity:\n",
    "                    if all(s >= confidence_threshold for s in current_entity[\"scores\"]):\n",
    "                        current_entity[\"text\"] = original_text[current_entity[\"start\"]:current_entity[\"end\"]]\n",
    "                        current_entity[\"score\"] = sum(current_entity[\"scores\"]) / len(current_entity[\"scores\"])\n",
    "                        del current_entity[\"scores\"]\n",
    "                        entities.append(current_entity)\n",
    "                    current_entity = None\n",
    "                tag_type = tag[2:]\n",
    "                current_entity = {\n",
    "                    \"start\": start,\n",
    "                    \"end\": end,\n",
    "                    \"tag\": tag_type,\n",
    "                    \"scores\": [score]\n",
    "                }\n",
    "\n",
    "            elif tag.startswith(\"I-\"):\n",
    "                tag_type = tag[2:]\n",
    "                if current_entity and current_entity[\"tag\"] == tag_type:\n",
    "                    current_entity[\"end\"] = end\n",
    "                    current_entity[\"scores\"].append(score)\n",
    "                else:\n",
    "                    current_entity = {\n",
    "                        \"start\": start,\n",
    "                        \"end\": end,\n",
    "                        \"tag\": tag_type,\n",
    "                        \"scores\": [score]\n",
    "                    }\n",
    "\n",
    "            else:  # \"O\"\n",
    "                if current_entity:\n",
    "                    if all(s >= confidence_threshold for s in current_entity[\"scores\"]):\n",
    "                        current_entity[\"text\"] = original_text[current_entity[\"start\"]:current_entity[\"end\"]]\n",
    "                        current_entity[\"score\"] = sum(current_entity[\"scores\"]) / len(current_entity[\"scores\"])\n",
    "                        del current_entity[\"scores\"]\n",
    "                        entities.append(current_entity)\n",
    "                    current_entity = None\n",
    "\n",
    "        if current_entity:\n",
    "            if all(s >= confidence_threshold for s in current_entity[\"scores\"]):\n",
    "                current_entity[\"text\"] = original_text[current_entity[\"start\"]:current_entity[\"end\"]]\n",
    "                current_entity[\"score\"] = sum(current_entity[\"scores\"]) / len(current_entity[\"scores\"])\n",
    "                del current_entity[\"scores\"]\n",
    "                entities.append(current_entity)\n",
    "\n",
    "        return entities\n",
    "\n",
    "\n",
    "    def do_prediction(self, text, confidence_threshold=0.6):\n",
    "        final_prediction = []\n",
    "        for sub_text, sub_text_start, sub_text_end in self.split_text_with_indices(text):\n",
    "            tokens = self.predict_text(text=sub_text, confidence_threshold=confidence_threshold)\n",
    "            predictions = self.aggregate_entities(tokens, sub_text, confidence_threshold=confidence_threshold)\n",
    "\n",
    "\n",
    "            for pred in predictions:\n",
    "                pred[\"start\"] += sub_text_start\n",
    "                pred[\"end\"] += sub_text_start\n",
    "                final_prediction.append(pred)\n",
    "\n",
    "        final_prediction_dict = {\n",
    "            lab: [p for p in final_prediction if p[\"tag\"] == lab]\n",
    "            for lab in self.base_entity_types\n",
    "        }\n",
    "        merged_predictions = []\n",
    "        for label in self.base_entity_types:\n",
    "            merged_predictions.extend(final_prediction_dict[label])\n",
    "        return merged_predictions\n",
    "\n",
    "\n",
    "def evaluate(model_checkpoint, revision, root_path, lang, cat):\n",
    "\n",
    "    ner = PredictionNER(model_checkpoint=model_checkpoint, revision=revision)\n",
    "\n",
    "    # conver the predictions to ann format\n",
    "    tsv_file_path_test = os.path.join(root_path,  f\"test_cardioccc_{lang}_{cat}.tsv\")\n",
    "    test_files_root =  os.path.join(root_path, \"txt\")\n",
    "\n",
    "    test_df = load_tsv_to_dataframe(tsv_file_path_test)\n",
    "    prd_ann = []\n",
    "\n",
    "    for fn in tqdm(test_df['filename'].unique()):\n",
    "\n",
    "        with open(os.path.join(test_files_root, fn+\".txt\"), 'r', encoding='utf-8') as f:\n",
    "            document_text = f.read()\n",
    "            prds = ner.do_prediction(document_text, confidence_threshold=0.35)\n",
    "            for prd in prds:\n",
    "                prd_ann.append({\n",
    "                    \"filename\": fn,\n",
    "                    \"label\": prd[\"tag\"],\n",
    "                    \"ann_id\": \"NA\",\n",
    "                    \"start_span\": prd[\"start\"],\n",
    "                    \"end_span\": prd[\"end\"],\n",
    "                    \"text\": prd[\"text\"],\n",
    "                })\n",
    "\n",
    "    output_tsv_path = os.path.join(root_path, f\"pre_{model_checkpoint.split('/')[1]}_{revision}.tsv\")\n",
    "    write_annotations_to_file(prd_ann, output_tsv_path)\n",
    "    print(f\"output_tsv_path {output_tsv_path}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized model in DT4H-IE/CardioBERTa.en_EN_MED. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v3, deformable_detr, deit, depth_anything, depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, git, glm, glm4, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, textnet, time_series_transformer, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zamba2, zoedepth",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m cat \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m lang \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 292\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model_checkpoint, revision, root_path, lang, cat)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mevaluate\u001b[39m(model_checkpoint, revision, root_path, lang, cat):\n\u001b[1;32m--> 292\u001b[0m     ner \u001b[38;5;241m=\u001b[39m \u001b[43mPredictionNER\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;66;03m# conver the predictions to ann format\u001b[39;00m\n\u001b[0;32m    295\u001b[0m     tsv_file_path_test \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root_path,  \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_cardioccc_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcat\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 85\u001b[0m, in \u001b[0;36mPredictionNER.__init__\u001b[1;34m(self, model_checkpoint, revision)\u001b[0m\n\u001b[0;32m     82\u001b[0m MAX_LENGTH \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[0;32m     83\u001b[0m OVERLAPPING_LEN \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m12\u001b[39m \n\u001b[1;32m---> 85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     87\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m AutoModelForTokenClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     89\u001b[0m     model_checkpoint, revision\u001b[38;5;241m=\u001b[39mrevision\n\u001b[0;32m     90\u001b[0m )\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter\u001b[38;5;241m.\u001b[39mfrom_tiktoken_encoder(\n\u001b[0;32m     92\u001b[0m     encoding_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo200k_base\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     93\u001b[0m     separators\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     96\u001b[0m     chunk_overlap\u001b[38;5;241m=\u001b[39mOVERLAPPING_LEN,\n\u001b[0;32m     97\u001b[0m )\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\bert\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:966\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    964\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfor_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_dict)\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 966\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    967\u001b[0m             pretrained_model_name_or_path, trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    968\u001b[0m         )\n\u001b[0;32m    969\u001b[0m config_tokenizer_class \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtokenizer_class\n\u001b[0;32m    970\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\bert\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:1151\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m   1148\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(pretrained_model_name_or_path):\n\u001b[0;32m   1149\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m CONFIG_MAPPING[pattern]\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[1;32m-> 1151\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1152\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized model in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould have a `model_type` key in its \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, or contain one of the following strings \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1154\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min its name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(CONFIG_MAPPING\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1155\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Unrecognized model in DT4H-IE/CardioBERTa.en_EN_MED. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v3, deformable_detr, deit, depth_anything, depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, git, glm, glm4, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, textnet, time_series_transformer, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zamba2, zoedepth"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"DT4H-IE/CardioBERTa.en_EN_MED\"\n",
    "revision = \"2025-05-07_19-08-13-d46774ee\"\n",
    "root = \"dataset/English\" \n",
    "cat = \"med\"\n",
    "lang = \"en\"\n",
    "\n",
    "evaluate(model_checkpoint, revision, root, lang, cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
