{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa83cbb6-acd5-4b59-a5a3-aafcd7c1b9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from datasets import Dataset, DatasetDict, Features, Sequence, ClassLabel, Value\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def split_iob_into_sentences(iob_tags, min_words=5, max_words=20):\n",
    "    \"\"\"\n",
    "    Splits IOB tags into sub-sentences using NLTK's sentence tokenizer.\n",
    "    Ensures that sentences have a length between min_words and max_words.\n",
    "\n",
    "    Args:\n",
    "        iob_tags (list of str): A list of strings in the format \"word<TAB>tag\".\n",
    "        min_words (int): Minimum number of words per sentence.\n",
    "        max_words (int): Maximum number of words per sentence.\n",
    "\n",
    "    Returns:\n",
    "        list of list of str: A list of sub-sentences, where each sub-sentence is a list of \"word<TAB>tag\" strings.\n",
    "    \"\"\"\n",
    "    # Combine the words into a single text string\n",
    "    text = ' '.join([word_tag.split('\\t')[0] for word_tag in iob_tags])\n",
    "    \n",
    "    # Use NLTK to tokenize the text into sentences\n",
    "    raw_sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Adjust sentence lengths based on thresholds\n",
    "    sentences = []\n",
    "    buffer = []\n",
    "    for sentence in raw_sentences:\n",
    "        words = sentence.split()\n",
    "        buffer.extend(words)\n",
    "        \n",
    "        if len(buffer) >= min_words:\n",
    "            sentences.append(' '.join(buffer[:max_words]))\n",
    "            buffer = buffer[max_words:]\n",
    "    \n",
    "    if buffer:\n",
    "        if sentences and len(sentences[-1].split()) + len(buffer) <= max_words:\n",
    "            sentences[-1] += ' ' + ' '.join(buffer)\n",
    "        else:\n",
    "            sentences.append(' '.join(buffer))\n",
    "    \n",
    "    # Reconstruct the IOB tags for each sentence\n",
    "    sub_sentences = []\n",
    "    word_index = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_words = sentence.split()\n",
    "        current_sentence = []\n",
    "\n",
    "        while word_index < len(iob_tags):\n",
    "            word, tag = iob_tags[word_index].split('\\t')\n",
    "            current_sentence.append(f\"{word}\\t{tag}\")\n",
    "            word_index += 1\n",
    "\n",
    "            if word in sentence_words:\n",
    "                sentence_words.remove(word)\n",
    "                if not sentence_words:\n",
    "                    break\n",
    "\n",
    "        sub_sentences.append(current_sentence)\n",
    "\n",
    "    return sub_sentences\n",
    "    \n",
    "\n",
    "def write_iob_to_file(iob_sentences, output_file_path):\n",
    "    \"\"\"\n",
    "    Writes IOB annotations to a file.\n",
    "\n",
    "    Args:\n",
    "        iob_sentences (list of list of str): A list of sentences, where each sentence is a list of strings\n",
    "                                            in the format \"word<TAB>tag\".\n",
    "        output_file_path (str): The path to the output file where the IOB annotations will be written.\n",
    "    \"\"\"\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "        for sentence in iob_sentences:\n",
    "            for word_tag in sentence:\n",
    "                file.write(word_tag + '\\n')\n",
    "            # Add an empty line after each sentence\n",
    "            file.write('\\n')\n",
    "\n",
    "def filter_entities(entities):\n",
    "    # Sort entities by start index, then by end index (longest first)\n",
    "    entities.sort(key=lambda x: (x[0], -x[1]))\n",
    "\n",
    "    # Filter entities to remove internal ranges\n",
    "    filtered_entities = []\n",
    "    for entity in entities:\n",
    "        start, end, entity_name, entity_text = entity\n",
    "        is_internal = False\n",
    "\n",
    "        # Check if the current entity is internal to any previously added entity\n",
    "        for prev_entity in filtered_entities:\n",
    "            prev_start, prev_end, _, _ = prev_entity\n",
    "            if start >= prev_start and end <= prev_end:\n",
    "                is_internal = True\n",
    "                break\n",
    "\n",
    "        # If the entity is not internal, add it to the filtered list\n",
    "        if not is_internal:\n",
    "            filtered_entities.append(entity)\n",
    "\n",
    "    return filtered_entities\n",
    "    \n",
    "\n",
    "\n",
    "def convert_conll_to_datasetdict(train_path, val_path=None, test_path=None, label_list=None, unknown_tag=\"O\"):\n",
    "    \"\"\"\n",
    "    Converts CoNLL files to a HuggingFace DatasetDict with typed features.\n",
    "    If unknown tags are found, they will be replaced with `unknown_tag`.\n",
    "    \"\"\"\n",
    "    if not label_list:\n",
    "        raise ValueError(\"You must provide a label_list to define features properly.\")\n",
    "\n",
    "    features = Features({\n",
    "        \"id\": Value(\"string\"),\n",
    "        \"tokens\": Sequence(Value(\"string\")),\n",
    "        \"ner_tags\": Sequence(ClassLabel(names=label_list))\n",
    "    })\n",
    "\n",
    "    label2id = {label: idx for idx, label in enumerate(label_list)}\n",
    "\n",
    "    def encode_tags(example):\n",
    "        corrected_tags = []\n",
    "        for tag in example[\"ner_tags\"]:\n",
    "            if tag not in label2id:\n",
    "                print(f\"Warning: unknown tag '{tag}' found. Replacing with '{unknown_tag}'\")\n",
    "                tag = unknown_tag\n",
    "            corrected_tags.append(label2id[tag])\n",
    "        example[\"ner_tags\"] = corrected_tags\n",
    "        return example\n",
    "\n",
    "    data_dict = {}\n",
    "    for split, path in zip([\"train\", \"validation\", \"test\"], [train_path, val_path, test_path]):\n",
    "        if path:\n",
    "            examples = parse_conll_file(path)\n",
    "            dataset = Dataset.from_list(examples)\n",
    "            dataset = dataset.map(encode_tags)\n",
    "            dataset = dataset.cast(features)  # Apply typed features after mapping\n",
    "            data_dict[split] = dataset\n",
    "\n",
    "    return DatasetDict(data_dict)\n",
    "\n",
    "def parse_conll_file(file_path):\n",
    "    \"\"\"\n",
    "    Parses a CoNLL file and returns a list of dicts with 'id', 'tokens', and 'ner_tags'.\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    with open(file_path, encoding='utf-8') as f:\n",
    "        tokens = []\n",
    "        tags = []\n",
    "        example_id = 0\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if tokens:\n",
    "                    examples.append({\n",
    "                        \"id\": str(example_id),\n",
    "                        \"tokens\": tokens,\n",
    "                        \"ner_tags\": tags\n",
    "                    })\n",
    "                    example_id += 1\n",
    "                    tokens = []\n",
    "                    tags = []\n",
    "            else:\n",
    "                splits = line.split()\n",
    "                if len(splits) >= 2:\n",
    "                    token = splits[0]\n",
    "                    tag = splits[-1]\n",
    "                    tokens.append(token)\n",
    "                    tags.append(tag)\n",
    "        if tokens:\n",
    "            examples.append({\n",
    "                \"id\": str(example_id),\n",
    "                \"tokens\": tokens,\n",
    "                \"ner_tags\": tags\n",
    "            })\n",
    "    return examples\n",
    "\n",
    "def ann2iob_singlefile(text_file_path, annotations):\n",
    "    # Read the text file\n",
    "    with open(text_file_path, 'r', encoding='utf-8-sig') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Parse annotations\n",
    "    entities = []\n",
    "    for ann in annotations:\n",
    "        try:\n",
    "            \n",
    "            entity_name_start_end = int(ann['start_span'])\n",
    "            entity_name_end_end = int(ann['end_span'])\n",
    "            entity_text = ann['text']\n",
    "            entity_type = ann['label']\n",
    "            \n",
    "            entities.append((entity_name_start_end, entity_name_end_end, entity_type, entity_text))\n",
    "        except Exception as ex:\n",
    "            print(ann)\n",
    "            print(ex)\n",
    "            break \n",
    "            \n",
    "    # Sort entities by start index, and then by length (longer first)\n",
    "    entities.sort(key=lambda x: (x[0], -x[1]))\n",
    "\n",
    "    entities = filter_entities(entities)\n",
    "\n",
    "    # Initialize IOB tags\n",
    "    iob_tags = ['O'] * len(text)\n",
    "\n",
    "    # Apply IOB tags\n",
    "    # print(entities)\n",
    "    for start, end, entity_name, entity_text in entities:\n",
    "        if 'anfetamínicos' in entity_text:\n",
    "            pass\n",
    "        # Find the word boundaries within the entity span\n",
    "        entity_words =  list(re.finditer(r'([0-9A-Za-zÀ-ÖØ-öø-ÿ]+|[^0-9A-Za-zÀ-ÖØ-öø-ÿ])', entity_text)) # list(re.finditer(r'\\S+', entity_text)) #  \n",
    "        for i, entity_word in enumerate(entity_words):\n",
    "            if not entity_word.group().strip():\n",
    "                continue\n",
    "            word_start = start + entity_word.start()\n",
    "            word_end = start + entity_word.end()\n",
    "            if i == 0:\n",
    "                iob_tags[word_start:word_end] = ['B-' + entity_name] * len(entity_word.group())# (word_end - word_start)\n",
    "            else:\n",
    "                iob_tags[word_start:word_end] = ['I-' + entity_name] * len(entity_word.group())# (word_end - word_start)\n",
    "\n",
    "    # Convert the text and IOB tags into word-level IOB format\n",
    "    text_words = list(re.finditer(r'([0-9A-Za-zÀ-ÖØ-öø-ÿ]+|[^0-9A-Za-zÀ-ÖØ-öø-ÿ])', text)) #re.finditer(r'\\w+|[^\\w\\s]', text)\n",
    "    iob_output = []\n",
    "    for text_word in text_words:\n",
    "        word_start = text_word.start()\n",
    "        word_end = text_word.end()\n",
    "        word_text = text[word_start:word_end]\n",
    "        if not word_text.strip():\n",
    "            continue\n",
    " \n",
    "        word_letters_tags = iob_tags[word_start:word_end]\n",
    "        if len(set(word_letters_tags))==1:\n",
    "            word_tag = iob_tags[word_start]\n",
    "        else:\n",
    "            word_tag = list(set(word_letters_tags).difference(\"O\"))[0]\n",
    "        iob_output.append(f\"{word_text}\\t{word_tag}\")\n",
    "\n",
    "    return iob_output\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def load_tsv_to_dataframe(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads a TSV file with specific columns into a pandas DataFrame.\n",
    "\n",
    "    Expected columns:\n",
    "        filename, label, start_span, end_span, text, note\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the TSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the TSV data.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(\n",
    "        file_path,\n",
    "        sep='\\t',\n",
    "        dtype={\n",
    "            \"filename\": str,\n",
    "            \"label\": str,\n",
    "            \"start_span\": int,\n",
    "            \"end_span\": int,\n",
    "            \"text\": str,\n",
    "            \"note\": str\n",
    "        },\n",
    "        keep_default_na=False  # Prevents empty strings being converted to NaN\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_iob(txt_root_dict, tsv_file_path, iob_file_path):\n",
    "    # to save the IOB sentences of all the files\n",
    "    # load all annotations\n",
    "    df = load_tsv_to_dataframe(tsv_file_path)\n",
    "\n",
    "    iob_sentences = []\n",
    "    for sample_name in tqdm(os.listdir(txt_root_dict)):\n",
    "\n",
    "\n",
    "        sample_text_file_path = os.path.join(txt_root_dict, sample_name)\n",
    "\n",
    "        # get the annotation of this specific sample:\n",
    "        sample_df = df[df['filename']==sample_name.replace(\".txt\", \"\")]\n",
    "        if len(sample_df)==0:\n",
    "            continue\n",
    "        sample_annotation = sample_df.to_dict(orient=\"records\")\n",
    "\n",
    "        # convert the annotation to IOB (all text):\n",
    "        iob_all_text = ann2iob_singlefile(text_file_path = sample_text_file_path, \n",
    "                                        annotations = sample_annotation)\n",
    "        # split the IOB into sentences:\n",
    "        iob_sentences_single_file = split_iob_into_sentences(iob_all_text, min_words=128, max_words=256)\n",
    "\n",
    "\n",
    "        iob_sentences.extend(iob_sentences_single_file)\n",
    "        \n",
    "        # break\n",
    "    write_iob_to_file(iob_sentences, iob_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfe4623b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sv\n",
      "dis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/508 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 508/508 [00:01<00:00, 313.64it/s]\n",
      "100%|██████████| 508/508 [00:01<00:00, 418.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "med\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 508/508 [00:01<00:00, 441.96it/s]\n",
      "100%|██████████| 508/508 [00:01<00:00, 491.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 508/508 [00:01<00:00, 369.62it/s]\n",
      "100%|██████████| 508/508 [00:01<00:00, 413.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "symp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 508/508 [00:01<00:00, 355.92it/s]\n",
      "100%|██████████| 508/508 [00:01<00:00, 401.62it/s]\n"
     ]
    }
   ],
   "source": [
    "cardio_ds_langs = {\n",
    "    # \"es\":\"Spanish\",\n",
    "    # \"en\": \"English\",\n",
    "    # \"cz\": \"Czech\",\n",
    "    # \"nl\": \"Dutch\",\n",
    "    # \"it\": \"Italian\",\n",
    "    # \"ro\": \"Romanian\",\n",
    "    \"sv\":\"Swedish\"\n",
    "}\n",
    "\n",
    "for lang_code, lang_name in cardio_ds_langs.items():\n",
    "    print(lang_code)\n",
    "    root = f\"../dataset/{lang_name}\"\n",
    "    lang = f\"{lang_code}\"\n",
    "\n",
    "\n",
    "    label_dict = {\n",
    "        \"dis\": [\"B-DISEASE\", \"I-DISEASE\", \"O\"],\n",
    "        \"med\": ['B-MEDICATION', 'I-MEDICATION', 'O'],\n",
    "        \"proc\": ['B-PROCEDURE', 'I-PROCEDURE', 'O'],\n",
    "        \"symp\": ['B-SYMPTOM', 'I-SYMPTOM', 'O'],\n",
    "    }\n",
    "    for cat in label_dict.keys():\n",
    "        print(cat)\n",
    "        label_list = label_dict[cat]  \n",
    "\n",
    "\n",
    "        # path to all .txt files\n",
    "        txt_root_dict = os.path.join(root, \"txt\")\n",
    "\n",
    "        # path to train/test annotations\n",
    "        tsv_file_path_train = os.path.join(root, f\"train_cardioccc_{lang}_{cat}.tsv\")\n",
    "        tsv_file_path_test = os.path.join(root,  f\"test_cardioccc_{lang}_{cat}.tsv\")\n",
    "\n",
    "        # path to save the IOB files\n",
    "        iob_file_path_train = os.path.join(root, f\"train_cardioccc_{lang}_{cat}.iob\")\n",
    "        iob_file_path_test = os.path.join(root, f\"test_cardioccc_{lang}_{cat}.iob\")\n",
    "\n",
    "\n",
    "        generate_iob(txt_root_dict, tsv_file_path_train, iob_file_path_train)\n",
    "        generate_iob(txt_root_dict, tsv_file_path_test, iob_file_path_test)\n",
    "\n",
    "\n",
    "        # HF_dataset = convert_conll_to_datasetdict(iob_file_path_train, test_path= iob_file_path_test, label_list=label_list)\n",
    "        # HF_dataset\n",
    "    # ds.save_to_disk(r\"dataset/processed_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd830f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
